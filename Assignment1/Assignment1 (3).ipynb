{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwO-S4LJ0HX_",
        "outputId": "b6b83b5a-79de-4fce-fea4-77762a194bee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184489 sha256=4e325c505737fcac5e00d9b818d03c51321a00977d1bed28ca441b536ba458d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9YqqZ63fzhoC"
      },
      "outputs": [],
      "source": [
        "import docx\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tag import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "\n",
        "def read_word_file(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    content = [para.text for para in doc.paragraphs]\n",
        "    return \"\\n\".join(content)"
      ],
      "metadata": {
        "id": "2_-4LD2M08QU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/Standard_Service_Agreement[1].docx\"\n",
        "text_content = read_word_file(file_path)"
      ],
      "metadata": {
        "id": "BFWy6SvB0iz3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "SdkdE1Kf1JNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_count(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return len(tokens), tokens"
      ],
      "metadata": {
        "id": "CEiSG0ee1GnL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop-word removal and count words"
      ],
      "metadata": {
        "id": "OzdvrcH01cGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words_and_count(words):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return len(filtered_words), filtered_words"
      ],
      "metadata": {
        "id": "3HZA2K911V-B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build vocabulary (unique words) and find its size"
      ],
      "metadata": {
        "id": "gUwP6vN01hb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(words):\n",
        "    vocabulary = set(words)\n",
        "    return len(vocabulary), vocabulary"
      ],
      "metadata": {
        "id": "oQLwpvnq1Zzh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming and count words"
      ],
      "metadata": {
        "id": "FJobhZws1oXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_and_count(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return len(stemmed_words), stemmed_words"
      ],
      "metadata": {
        "id": "RBtuJgYC1mZY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization and count words"
      ],
      "metadata": {
        "id": "4OpxREvZ1vzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_and_count(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return len(lemmatized_words), lemmatized_words"
      ],
      "metadata": {
        "id": "_2Ma42wu1rhw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get most frequent words after stop-word removal"
      ],
      "metadata": {
        "id": "A4_Pvoe_14yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_frequent_words(words, n=10):\n",
        "    fdist = FreqDist(words)\n",
        "    most_frequent = fdist.most_common(n)\n",
        "    return most_frequent"
      ],
      "metadata": {
        "id": "WBVQeicf13ca"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count Nouns and Adjectives after stemming/lemmatization"
      ],
      "metadata": {
        "id": "UIQoOEAb2BwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_nouns_and_adjectives(words):\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = len([word for word, tag in tagged_words if tag.startswith('NN')])\n",
        "    adjectives = len([word for word, tag in tagged_words if tag.startswith('JJ')])\n",
        "    return nouns, adjectives"
      ],
      "metadata": {
        "id": "HgyMprb018WS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RsWGLc63F8f",
        "outputId": "c4b48322-6b5b-43d4-b6bd-63698b7ce809"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwAB_97I3W-d",
        "outputId": "a6669cfa-17f7-4dba-c7ce-67f4642db9e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UdDWWg53cFv",
        "outputId": "98180fbd-234a-4ad8-afe2-53380788e10e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7eCuyuP3oC-",
        "outputId": "706f7663-9a9d-4489-d34d-dc98748c4892"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"/content/Standard_Service_Agreement[1].docx\"\n",
        "# Read the content of the Word file\n",
        "text_content = read_word_file(input_file_path)\n",
        "# Tokenization and count tokens\n",
        "num_tokens, tokens = tokenize_and_count(text_content)\n",
        "# Stop-word removal and count words\n",
        "num_words_without_stopwords, words_without_stopwords = remove_stop_words_and_count(tokens)\n",
        "# Build vocabulary and find its size\n",
        "vocab_size, vocabulary = build_vocabulary(words_without_stopwords)\n",
        "# Stemming and count words\n",
        "num_words_stemmed, stemmed_words = stem_and_count(words_without_stopwords)\n",
        "# Lemmatization and count words\n",
        "num_words_lemmatized, lemmatized_words = lemmatize_and_count(words_without_stopwords)\n",
        "# Get 10 most frequent words after stop-word removal\n",
        "most_frequent_words = get_most_frequent_words(words_without_stopwords, n=10)\n",
        "# Count Nouns and Adjectives after stemming/lemmatization\n",
        "num_nouns, num_adjectives = count_nouns_and_adjectives(stemmed_words)"
      ],
      "metadata": {
        "id": "iIIn8PYI2M-f"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "print(\"Number of Tokens:\", num_tokens)\n",
        "print(\"Number of Words (after stop-word removal):\", num_words_without_stopwords)\n",
        "print(\"Vocabulary Size:\", vocab_size)\n",
        "print(\"Number of Words (after stemming):\", num_words_stemmed)\n",
        "print(\"Number of Words (after lemmatization):\", num_words_lemmatized)\n",
        "print(\"Most 10 Frequent Words:\", most_frequent_words)\n",
        "print(\"Number of Nouns:\", num_nouns)\n",
        "print(\"Number of Adjectives:\", num_adjectives)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyghUQDn2U7g",
        "outputId": "af2eb1f2-a569-40dd-e57c-ac9349bee2f6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tokens: 9401\n",
            "Number of Words (after stop-word removal): 5838\n",
            "Vocabulary Size: 1465\n",
            "Number of Words (after stemming): 5838\n",
            "Number of Words (after lemmatization): 5838\n",
            "Most 10 Frequent Words: [(',', 542), ('Service', 197), ('Provider', 192), ('.', 176), ('NetApp', 172), ('shall', 137), ('Agreement', 116), (')', 109), ('(', 105), ('Information', 48)]\n",
            "Number of Nouns: 3016\n",
            "Number of Adjectives: 881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yDQx2dAs3-6G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}