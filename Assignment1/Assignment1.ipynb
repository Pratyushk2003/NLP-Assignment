{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eWmiN_ioGm4",
        "outputId": "3d022e58-38a6-4715-9322-4da60278662c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import docx\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Download the stopwords corpus (run this only once)\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "\n",
        "def read_word_file(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    content = [para.text for para in doc.paragraphs]\n",
        "    return \"\\n\".join(content)\n"
      ],
      "metadata": {
        "id": "y6JXEU7LoPt7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/Standard_Service_Agreement[1].docx\"\n",
        "text_content = read_word_file(file_path)"
      ],
      "metadata": {
        "id": "ZNp-tx_kqQSG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowercasing"
      ],
      "metadata": {
        "id": "x_MIAyOeoS4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_lowercase(text):\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "7yLIOMPmoUSh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "uLlgJgf4oio-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVRHP2xvoc8Q",
        "outputId": "e6eeabcb-2db8-40a1-9e7f-3b32427b8883"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Punctuation and Special Characters"
      ],
      "metadata": {
        "id": "uhx0Kmkeor4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_special_characters(tokens):\n",
        "    return [re.sub(r'[^\\w\\s]', '', token) for token in tokens]"
      ],
      "metadata": {
        "id": "8Nj85693ossN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stop Words"
      ],
      "metadata": {
        "id": "xjIZu8NLoulJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [token for token in tokens if token not in stop_words]\n"
      ],
      "metadata": {
        "id": "0Iou3s0-o0Vr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization or Stemming"
      ],
      "metadata": {
        "id": "9fSHu4SMo3y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]"
      ],
      "metadata": {
        "id": "jUM7x-rjo6xv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = convert_to_lowercase(text)\n",
        "    tokens = tokenize_text(text)\n",
        "    tokens = remove_special_characters(tokens)\n",
        "    tokens = remove_stop_words(tokens)\n",
        "    tokens = lemmatize_tokens(tokens)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "6rUwc5pBo8zP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"/content/Standard_Service_Agreement[1].docx\"\n",
        "output_file_path = \"/content/preprocessed_output.txt\"\n",
        "\n",
        "# Read the content of the Word file\n",
        "text_content = read_word_file(input_file_path)\n"
      ],
      "metadata": {
        "id": "uJ7kejfyrPkj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "preprocessed_text = preprocess_text(text_content)\n",
        "\n",
        "# Join the preprocessed tokens to create a string\n",
        "preprocessed_text_str = \" \".join(preprocessed_text)\n",
        "\n",
        "# Write the preprocessed text to a new file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    output_file.write(preprocessed_text_str)\n",
        "\n",
        "print(\"Preprocessed text has been saved to:\", output_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seorY00ws8LS",
        "outputId": "f5ee4c59-0cac-40b0-c3e1-8d00e8188a00"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed text has been saved to: /content/preprocessed_output.txt\n"
          ]
        }
      ]
    }
  ]
}